{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3yFWkzxgdP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import argparse\n",
        "from smart_open import smart_open\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from tqdm import tqdm_notebook\n",
        "import inflect \n",
        "from gensim.models.fasttext import FastText\n",
        "from scipy.spatial.distance import cosine\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Im-c7GlDe4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprcessing for Fast Text  \n",
        "nltk.download(\"wordnet\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "p = inflect.engine() \n",
        "stops = list(stopwords.words(\"english\"))\n",
        "extra = [']','[','(',')','{','@','}',';',':','#','$','^','+','-',',','=','.']\n",
        "print(len(stops))\n",
        "\n",
        "def remove_punctuation(text): \n",
        "\ttranslator = str.maketrans('', '', string.punctuation) \n",
        "\treturn text.translate(translator) \n",
        "def remove_whitespace(text): \n",
        "\treturn \" \".join(text.split()) \n",
        "def remove_others(text):\n",
        "    st = \"\"\n",
        "    for i in range(0,len(text)):\n",
        "      if(text[i] not in extra):\n",
        "        st+=text[i]\n",
        "    return st\n",
        "def lemmetize(text):\n",
        "    ans = \"\"\n",
        "    g = text.split(' ')\n",
        "    for x in g:\n",
        "      ans += \"{} \".format(lemmatizer.lemmatize(x))\n",
        "    return ans\n",
        "def convert_number(text): \n",
        "\ttemp_str = text.split() \n",
        "\tnew_string = [] \n",
        "\n",
        "\tfor word in temp_str: \n",
        "\t\tif word.isdigit(): \n",
        "\t\t\ttemp = p.number_to_words(word) \n",
        "\t\t\tnew_string.append(temp) \n",
        "\t\telse: \n",
        "\t\t\tnew_string.append(word) \n",
        "\ttemp_str = ' '.join(new_string) \n",
        "\treturn temp_str \n",
        "\n",
        "def remove_stopwords(text):\n",
        "  g = text.split(' ')\n",
        "  ans = \"\"\n",
        "  for i in range(0,len(g)):\n",
        "    if(g[i] not in stops):\n",
        "      if(i==len(g)-1):\n",
        "        ans+=\"{}\".format(g[i])\n",
        "      else:\n",
        "        ans+=\"{} \".format(g[i])\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tok1NfNT-tuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "def display_closestwords_tsnescatterplot(model, word, size):\n",
        "    arr = np.empty((0,size), dtype='f')\n",
        "    word_labels = [word]\n",
        "    close_words = model.similar_by_word(word)\n",
        "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
        "    for wrd_score in close_words:\n",
        "      wrd_vector = model[wrd_score[0]]\n",
        "      word_labels.append(wrd_score[0])\n",
        "      arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
        "        \n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    Y = tsne.fit_transform(arr)\n",
        "    x_coords = Y[:, 0]\n",
        "    y_coords = Y[:, 1]\n",
        "    plt.scatter(x_coords, y_coords)\n",
        "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
        "      plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "    plt.xlim(x_coords.min()+0.0005, x_coords.max()+0.0005)\n",
        "    plt.ylim(y_coords.min()+0.0005, y_coords.max()+0.0005)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeW9IG9u93js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def word2vec2tensor(word2vec_model_path, tensor_filename, binary=False):\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=binary, unicode_errors='ignore')\n",
        "    outfiletsv = tensor_filename + '_tensor.tsv'\n",
        "    outfiletsvmeta = tensor_filename + '_metadata.tsv'\n",
        "\n",
        "    with smart_open(outfiletsv, 'wb') as file_vector, smart_open(outfiletsvmeta, 'wb') as file_metadata:\n",
        "        for word in model.index2word:\n",
        "            file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\\n'))\n",
        "            vector_row = '\\t'.join(str(x) for x in model[word])\n",
        "            file_vector.write(gensim.utils.to_utf8(vector_row) + gensim.utils.to_utf8('\\n'))\n",
        "\n",
        "    logger.info(\"2D tensor file saved to %s\", outfiletsv)\n",
        "    logger.info(\"Tensor metadata file saved to %s\", outfiletsvmeta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-uV4scQgo5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xpt_8cMM1S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceIterator: \n",
        "    def __init__(self, filepath): \n",
        "        self.filepath = filepath \n",
        "\n",
        "    def __iter__(self): \n",
        "        for line in open(self.filepath): \n",
        "            yield line.split() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTjPm0k_OgWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip gdrive/My\\ Drive/pubmed-rct-master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJiRrtrOpspK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!7z e pubmed-rct-master/PubMed_200k_RCT/train.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cfOPsVhGc6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m nltk.downloader stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYKVkSZHF6Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "#word_tokenize accepts a string as an input, not a file. \n",
        "stop_words = set(stopwords.words('english')) \n",
        "file1 = open(\"train.txt\") \n",
        "line = file1.read()# Use this to read file content as a stream: \n",
        "words = line.split() \n",
        "for r in tqdm_notebook(words):   \n",
        "    if not r in stop_words: \n",
        "        appendFile = open('lowercasetext.txt','a') \n",
        "        appendFile.write(\" \"+r.lower()) \n",
        "        appendFile.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhJvEixKIXIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIWS5O09OwK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = SentenceIterator('train.txt')\n",
        "model = Word2Vec(sentences)\n",
        "# Change window size and min word count\n",
        "model = Word2Vec(sentences, min_count=5, workers=3, window =5, sg = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPD013o3Y68Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = KeyedVectors.load_word2vec_format('Cbow_default')\n",
        "model2 = KeyedVectors.load_word2vec_format('Skipgram_default')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr3xqmT0Itia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.most_similar('cardiovascular',topn = 15)\n",
        "model.most_similar('skin',topn = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzSS01l99p-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.save_word2vec_format('Cbow_default')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BbIQ3WMM7Vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp Cbow_default gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWVEEWjELsCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp GoogleNews-vectors-negative300.bin.gz gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjsUp7GOAzYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gdrive/My\\ Drive/lowercasetext.txt ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYtsbkIMB_N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.cosine_similarities(model['illness'], [model['disease']])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fykh4rz0D_2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_file = open('words_classes.txt', 'a+')\n",
        "i = 0\n",
        "for k in tqdm_notebook(words):\n",
        "  x = model.wv.cosine_similarities(model[k], [model['disease']])[0]\n",
        "  y = model.wv.cosine_similarities(model[k], [model['symptom']])[0]\n",
        "  z = model.wv.cosine_similarities(model[k], [model['drug']])[0]\n",
        "  if(x == max(x,y,z)):\n",
        "    txt_file.write(k + \" \" + str(1) + \"\\n\")\n",
        "    continue\n",
        "  elif(y == max(x,y,z)):\n",
        "    txt_file.write(k + \" \" + str(2)+ \"\\n\")\n",
        "    continue\n",
        "  else:\n",
        "    txt_file.write(k + \" \" + str(3)+ \"\\n\")\n",
        "    continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLfm5QJT13-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.most_similar(positive=['neuro', 'heart'], negative=['cardiovascular'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEpBYKmXloEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences2 = SentenceIterator('train.txt')\n",
        "model2 = Word2Vec(sentences2, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy36TuLCBNRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.most_similar('skin', topn=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O65FfPN9qetV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.wv.save_word2vec_format('Skipgram_default')\n",
        "word2vec2tensor('Skipgram_default', 'Skipgram_default', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN6UbLOiqlS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp Skipgram_default_* gdrive/My\\ Drive/\n",
        "!cp Skipgram_default gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfiqYyNlxUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.most_similar('cardiovascular')[0:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl28qqoWBNZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_file = datapath(\"/content/glove.6B.100d.txt\")\n",
        "tmp_file = get_tmpfile(\"glove_embeddings_word2vec.txt\")\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fYNqIDoDibd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_word2vec_format(\"pretrained_glove\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5JmNYYfD3pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp pretrained_glove gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHKwFa4E7EDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2 = Word2Vec(size=100, min_count=5)\n",
        "model_2.build_vocab(sentences)\n",
        "total_examples = model_2.corpus_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVQU__I1C8dH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(total_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyPj19VGEEQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"pretrained_glove_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w39ilQD1E6Cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_model = Word2Vec.load(\"pretrained_glove_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ftX8fcD7EA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.build_vocab([list(model.vocab.keys())], update=True)\n",
        "model.intersect_word2vec_format(\"glove.6B.100d.txt\", binary=False, lockf=1.0)\n",
        "model.train(sentences, total_examples=total_examples, epochs=model_2.iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDaFPDtijR5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install glove_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mjKykVGCOPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "#Creating a corpus object\n",
        "corpus = Corpus() \n",
        "\n",
        "#Training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "corpus.fit(sentences, window=15)\n",
        "glove = Glove(no_components=100, learning_rate=0.05) \n",
        "glove.fit(corpus.matrix, epochs=50, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove_30e_15w.model')\n",
        "!cp glove_30e_15w.model gdrive/My\\ Drive/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z64JpirWmVyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus() \n",
        "corpus.fit(sentences, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05) \n",
        "glove.fit(corpus.matrix, epochs=50, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove_30e_5w.model')\n",
        "!cp glove_30e_5w.model gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ulvc0X02TkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glove import Corpus, Glove\n",
        "glove2 = Glove.load('glove.model')\n",
        "glove2.dictionary.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xilWDbduLVAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KeyedVectors.cosine_similarities(glove2.word_vectors[glove2.dictionary['cardio']], [glove2.word_vectors[glove2.dictionary['and']]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMfSE6y9ozBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove2.word_vectors[glove2.dictionary['cardio']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YLbSZtUy1Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_filename = 'glove'\n",
        "outfiletsv = tensor_filename + '_tensor.tsv'\n",
        "outfiletsvmeta = tensor_filename + '_metadata.tsv'\n",
        "\n",
        "with smart_open(outfiletsv, 'wb') as file_vector, smart_open(outfiletsvmeta, 'wb') as file_metadata:\n",
        "    for word in glove2.dictionary.keys():\n",
        "        file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\\n'))\n",
        "        vector_row = '\\t'.join(str(x) for x in glove2.word_vectors[glove2.dictionary[word]])\n",
        "        file_vector.write(gensim.utils.to_utf8(vector_row) + gensim.utils.to_utf8('\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNt56Rn6COIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "def getAverage(x,word):\n",
        "  avg = 0.0\n",
        "  for i in x:\n",
        "    avg += cosine(model[i],model[word])https://www.google.com/search?client=ubuntu&channel=fs&q=mydrive&ie=utf-8&oe=utf-8\n",
        "  return avg/len(x)\n",
        "words = [['disease','dieseases','infectious-diseases','Disease'],['symptom','sign','symptoms','Symptom'], ['drug','drugs','medication']]\n",
        "final = []\n",
        "#out_file = open()\n",
        "for word in model.words:\n",
        "  p = 100\n",
        "  cur = None\n",
        "  for i in range(0,len(words)): \n",
        "    temp = getAverage(words[i],word)\n",
        "    if(p>temp):\n",
        "      cur = i \n",
        "      p=temp\n",
        "  if(cur==None):\n",
        "    print(temp,i,word)\n",
        "    break\n",
        "  final.append([word,cur])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ4pvhicOwF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_list = [['tetanus', 'spasms', 'Penicillin'], ['Rabies', 'hypersalivation', 'globulin'], ['Measles', 'rash', 'Analgesic'],\n",
        "             ['Asthma', 'cough', 'bronchodilator'], ['typhoid', 'fever', 'Penicillin'], ['Malaria', 'fever', 'chloroquine'],\n",
        "             ['Dengue', 'fever', 'Analgesic'], ['Flu', 'cough', 'decongestant'], ['Cardiomyopathy', 'breathlessness', 'Anticoagulant'],\n",
        "             ['achalasia', 'heartburn', 'Antianginal'], ['chickenpox', 'rash', 'Analgesic'], ['Cholera', 'diarrhoea', 'IV'],\n",
        "             ['Tuberculosis', 'cough', 'antibiotics']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCS4yG4yOwBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1['rash'] #CBOW\n",
        "model2['rash'] #Skipgram\n",
        "glove2.word_vectors[glove2.dictionary['cardio']] #glove\n",
        "eval_list_glove = []\n",
        "eval_list_cbow = []\n",
        "eval_list_skipgram = []\n",
        "for l in eval_list:\n",
        "  k = []\n",
        "  for w in l:\n",
        "    k.append(model1[w])\n",
        "  eval_list_cbow.append(k)\n",
        "for l in eval_list:\n",
        "  k = []\n",
        "  for w in l:\n",
        "    k.append(model2[w])\n",
        "  eval_list_skipgram.append(k)\n",
        "for l in eval_list:\n",
        "  k = []\n",
        "  for w in l:\n",
        "    k.append(glove2.word_vectors[glove2.dictionary['cardio']])\n",
        "  eval_list_glove.append(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjqV_NpZjxPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove2.word_vectors[glove2.dictionary['infectious-diseases']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL5nkMadizWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [['disease','diseases','Disease'],['symptom','sign','symptoms','Symptom'], ['drug','drugs','medication']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGQqnTqPhor6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "def getAverage_glove(x,word):\n",
        "  avg = 0.0\n",
        "  for i in x:\n",
        "    avg += cosine(glove2.word_vectors[glove2.dictionary[i]],word)\n",
        "  return avg/len(x)\n",
        "\n",
        "def getAverage_cbow(x,word):\n",
        "  avg = 0.0\n",
        "  for i in x:\n",
        "    avg += cosine(model1[i],word)\n",
        "  return avg/len(x)\n",
        "\n",
        "def getAverage_skipgram(x,word):\n",
        "  avg = 0.0\n",
        "  for i in x:\n",
        "    avg += cosine(model2[i],word)\n",
        "  return avg/len(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KDEMco4Ov-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluation metrics\n",
        "def metric_p1(eval_list_emb):\n",
        "  mp1 = 0\n",
        "  for w in eval_list_emb:\n",
        "    for i in range(len(eval_list_emb)):\n",
        "      for j in range(i+1,len(eval_list_emb)):\n",
        "        mp1+=cosine(eval_list_emb[i],eval_list_emb[j])\n",
        "  return mp1/=len(eval_list_emb)\n",
        "  \n",
        "def metric_p2(eval_list_emb):\n",
        "  mp2 = 0\n",
        "  for l in eval_list_emb:\n",
        "    for w in l:\n",
        "      #add one to the mp2 for correct classification \n",
        "  return mp2/=3*(len(eval_list_emb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5I3YEkAOv8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_p2_glove(eval_list_emb):\n",
        "  mp2 = 0\n",
        "  for l in eval_list_emb:\n",
        "      a1 = getAverage_glove(words[0], l[0])\n",
        "      a2 = getAverage_glove(words[1], l[0])\n",
        "      a3 = getAverage_glove(words[2], l[0])\n",
        "      b1 = getAverage_glove(words[0], l[1])\n",
        "      b2 = getAverage_glove(words[1], l[1])\n",
        "      b3 = getAverage_glove(words[2], l[1])\n",
        "      c1 = getAverage_glove(words[0], l[2])\n",
        "      c2 = getAverage_glove(words[1], l[2])\n",
        "      c3 = getAverage_glove(words[2], l[2])\n",
        "      if(a1 == max(a1,a2,a3)): mp2 += 1\n",
        "      if(b2 == max(b1,b2,b3)): mp2 += 1\n",
        "      if(c3 == max(c1,c2,c3)): mp2 += 1\n",
        "      #add one to the mp2 for correct classification\n",
        "  ans = mp2/(3*len(eval_list_emb))\n",
        "  return ans\n",
        "\n",
        "def metric_p2_cbow(eval_list_emb):\n",
        "  mp2 = 0\n",
        "  for l in eval_list_emb:\n",
        "      a1 = getAverage_cbow(words[0], l[0])\n",
        "      a2 = getAverage_cbow(words[1], l[0])\n",
        "      a3 = getAverage_cbow(words[2], l[0])\n",
        "      b1 = getAverage_cbow(words[0], l[1])\n",
        "      b2 = getAverage_cbow(words[1], l[1])\n",
        "      b3 = getAverage_cbow(words[2], l[1])\n",
        "      c1 = getAverage_cbow(words[0], l[2])\n",
        "      c2 = getAverage_cbow(words[1], l[2])\n",
        "      c3 = getAverage_cbow(words[2], l[2])\n",
        "      if(a1 == max(a1,a2,a3)): mp2 += 1\n",
        "      if(b2 == max(b1,b2,b3)): mp2 += 1\n",
        "      if(c3 == max(c1,c2,c3)): mp2 += 1\n",
        "      #add one to the mp2 for correct classification\n",
        "  ans = mp2/(3*len(eval_list_emb))\n",
        "  return ans\n",
        "\n",
        "def metric_p2_skipgram(eval_list_emb):\n",
        "  mp2 = 0\n",
        "  for l in eval_list_emb:\n",
        "      a1 = getAverage_skipgram(words[0], l[0])\n",
        "      a2 = getAverage_skipgram(words[1], l[0])\n",
        "      a3 = getAverage_skipgram(words[2], l[0])\n",
        "      b1 = getAverage_skipgram(words[0], l[1])\n",
        "      b2 = getAverage_skipgram(words[1], l[1])\n",
        "      b3 = getAverage_skipgram(words[2], l[1])\n",
        "      c1 = getAverage_skipgram(words[0], l[2])\n",
        "      c2 = getAverage_skipgram(words[1], l[2])\n",
        "      c3 = getAverage_skipgram(words[2], l[2])\n",
        "      if(a1 == max(a1,a2,a3)): mp2 += 1\n",
        "      if(b2 == max(b1,b2,b3)): mp2 += 1\n",
        "      if(c3 == max(c1,c2,c3)): mp2 += 1\n",
        "      #add one to the mp2 for correct classification\n",
        "  ans = mp2/(3*len(eval_list_emb))\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQnLciAT-EmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NER for diseases\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import unicodedata\n",
        " \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense\n",
        "from keras.layers import TimeDistributed, Dropout, Bidirectional\n",
        " \n",
        "# Defining Constants\n",
        " \n",
        "# Maximum length of text sentences\n",
        "MAXLEN = 180\n",
        "# Number of LSTM units\n",
        "LSTM_N = 150\n",
        "# batch size\n",
        "BS=48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n0sYtEw-EiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"train.csv\", encoding=\"latin1\")\n",
        "test_data = pd.read_csv(\"test.csv\", encoding=\"latin1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk9-HI6F-EhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = list(set(data[\"Word\"].append(test_data[\"Word\"]).values))\n",
        "words.append(\"ENDPAD\")\n",
        " \n",
        "# Converting greek characters to ASCII characters eg. 'naïve café' to 'naive cafe'\n",
        "words = [unicodedata.normalize('NFKD', str(w)).encode('ascii','ignore') for w in words]\n",
        "n_words = len(words)\n",
        "print(\"\\nLength of vocabulary = \",n_words)\n",
        " \n",
        "tags = list(set(data[\"tag\"].values))\n",
        "n_tags = len(tags)\n",
        "print(\"\\nnumber of tags = \",n_tags)\n",
        " \n",
        "# Creating words to indices dictionary.\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "# Creating tags to indices dictionary.\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3AcAhZ0-EfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tagged_sentences(data):\n",
        "'''\n",
        "Objective: To get list of sentences along with labelled tags.\n",
        "Returns a list of lists of (word,tag) tuples.\n",
        "Each inner list contains a words of a sentence along with tags.\n",
        "'''\n",
        "    agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"tag\"].values.tolist())]\n",
        "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
        "    sentences = [s for s in grouped]\n",
        "    return sentences\n",
        " \n",
        "def get_test_sentences(data):\n",
        "'''\n",
        "Objective: To get list of sentences.\n",
        "Returns a list of lists of words.\n",
        "Each inner list contains a words of a sentence.\n",
        "'''\n",
        " \n",
        "    agg_func = lambda s: [w for w in s[\"Word\"].values.tolist()]\n",
        "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
        "    sentences = [s for s in grouped]\n",
        "    return sentences\n",
        "# Getting training sentences in a list\n",
        "sentences = get_tagged_sentences(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAufL8Hb-Eb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting words to indices for test sentences (Features)\n",
        "# Converting greek characters to ASCII characters in train set eg. 'naïve café' to 'naive cafe'\n",
        "X = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).\n",
        "encode('ascii','ignore')] for w in s] for s in sentences]\n",
        " \n",
        "# Converting words to indices for test sentences (Features)\n",
        "# Converting greek characters to ASCII characters in test-set eg. 'naïve café' to 'naive cafe'\n",
        "X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).\n",
        "encode('ascii','ignore')] for w in s] for s in test_sentences]\n",
        " \n",
        "'''\n",
        "Padding train and test sentences to 180 words.\n",
        "Sentences of length greater than 180 words are truncated.\n",
        "Sentences of length less than 180 words are padded with a high value.\n",
        "'''\n",
        "X = pad_sequences(maxlen=MAXLEN, sequences=X, padding=\"post\", value=n_words - 1)\n",
        "X_test = pad_sequences(maxlen=MAXLEN, sequences=X_test, padding=\"post\", value=n_words - 1)\n",
        " \n",
        "# Converting tags to indices for test sentences (labels)\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "# Padding tag labels to 180 words.\n",
        "y = pad_sequences(maxlen=MAXLEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
        " \n",
        "# Making labels in one hot encoded form for DL model\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3DBeMNi-ERy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 180 dimensional word indices as input\n",
        "input = Input(shape=(MAXLEN,))\n",
        " \n",
        "# Embedding layer of same length output (180 dim embedding will be generated)\n",
        "model = Embedding(input_dim=n_words, output_dim=MAXLEN, input_length=MAXLEN)(input)\n",
        " \n",
        "# Adding dropout layer\n",
        "model = Dropout(0.2)(model)\n",
        " \n",
        "# Bidirectional LSTM to learn from both forward as well as backward context\n",
        "model = Bidirectional(LSTM(units=LSTM_N, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        " \n",
        "# Adding a TimeDistributedDense, to applying a Dense layer on each 180 timesteps\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model) # softmax output layer\n",
        "model = Model(input, out)\n",
        " \n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X, np.array(y), batch_size=BS, epochs=2, validation_split=0.05, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKRxgmYB_ljr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(X_test)\n",
        "pred_index = np.argmax(pred, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pF1bka1jdy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metric_p2_glove(eval_list_glove))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbDHvvg5kT0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metric_p2_cbow(eval_list_cbow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdp_YbmSlQtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metric_p2_skipgram(eval_list_skipgram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJqbbU-yDvPu",
        "colab_type": "text"
      },
      "source": [
        "Fast Text Based Model Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdc7gy59DrqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fasttext librry\n",
        "import fasttext\n",
        "model_skip = fasttext.train_unsupervised(\"/content/pubmed-rct-master/PubMed_20k_RCT_numbers_replaced_with_at_sign/train_pre.txt\",model='skipgram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkpasgWpEPdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_skip.get_nearest_neighbors(\"medicine\",20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h6YndH_EAzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.fasttext import FastText \n",
        "#Gensim Model Fasttext\n",
        "model = FastText(corpus_file=\"/content/pubmed-rct-master/PubMed_20k_RCT_numbers_replaced_with_at_sign/train_pre.txt\",e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6osFP0cBESLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_nearest_neighbors('sinusitis',20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TGZA35rEHe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code for cosine Similarity Fast Text\n",
        "from scipy.spatial.distance import cosine\n",
        "def getAverage(x,word):\n",
        "  avg = 0.0\n",
        "  for i in x:\n",
        "    avg += cosine(model_skip[i],model_skip[word])\n",
        "  return avg/len(x)\n",
        "words = [['disease','diseases','infectious-diseases'],['drug','medicine'],['symptom','sign']]\n",
        "final = []\n",
        "c =0\n",
        "for word in model_skip.words:\n",
        "  c+=1\n",
        "  p = 100\n",
        "  cur = None\n",
        "  for i in range(0,len(words)): \n",
        "    temp = getAverage(words[i],word)\n",
        "    if(p>temp):\n",
        "      cur = i \n",
        "      p=temp\n",
        "  if(cur==None):\n",
        "    print(temp,i,word)\n",
        "    break\n",
        "  final.append([word,cur])\n",
        "def getD(model,a,b):\n",
        "  return 1.0-cosine(model_skip[a],model_skip[b])\n",
        "p = [\"disease\",\"drug\",\"symptom\"]\n",
        "l = [\"aspirin\",\"paracetamol\",\"insomania\",\"diabetes\",\"thyroid\",\"insulin\"]\n",
        "for k in l:\n",
        "  ans = \"\"\n",
        "  for i in p:\n",
        "    ans += \"{} \".format(getD(model_skip,k,i))\n",
        "  print(k,ans)\n",
        "\n",
        "print(final)\n",
        "f = open(\"/content/drive/My Drive/labelsFastText.csv\",\"w\")\n",
        "f.write(\"{}\\n\".format(words))\n",
        "for x in final:\n",
        "  ans = \"{}, {}\\n\".format(x[0],x[1])\n",
        "  f.write(ans)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}